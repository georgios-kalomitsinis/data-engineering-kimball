{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ba4d1a-442c-4b1b-abab-ba21c2b8ffd1",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9e434a-e858-4abb-abfc-537a8d9e6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from functools import wraps\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6769b7-3362-457a-9228-d0a27ac6117b",
   "metadata": {},
   "source": [
    "Logging decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4bf1e6-7e84-4747-8d1e-f93e85d314b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"Starting: {func.__name__}\")\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = time.time() - start\n",
    "        print(f\"Completed: {func.__name__} in {duration:.2f} seconds\\n\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d67a6-099f-47ae-aff4-0d6c0addb6f3",
   "metadata": {},
   "source": [
    "Loading the params file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453a941e-e43b-4972-8660-a846765cc73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def load_json_config(json_path: str) -> dict:\n",
    "    with open(json_path, 'r') as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    config = raw[0] if isinstance(raw, list) else raw\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2eabd-0bc5-4ab5-b055-33ce7d811c91",
   "metadata": {},
   "source": [
    "Step 1: Extract raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82af303-b9ab-4b37-88a3-7f961a959895",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def extract_data(filepath: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "    df.columns = [col.strip().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "    # Trim all string columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f291d5-a869-41ed-b503-d572a3101281",
   "metadata": {},
   "source": [
    "Step 2: helping function to extract features for the DimTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fb34a6-4c7d-4804-b4b9-f365b019affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def assign_seasonality_from_description(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    seasonal_keywords = [\n",
    "        \"christmas\", \"xmas\", \"wreath\", \"snow\", \"advent\", \"stocking\", \"santa\",\n",
    "        \"holiday\", \"reindeer\", \"easter\", \"halloween\", \"valentine\", \"spring\",\n",
    "        \"summer\", \"autumn\", \"winter\"\n",
    "    ]\n",
    "\n",
    "    def is_seasonal(desc):\n",
    "        if not isinstance(desc, str):\n",
    "            return \"N\"\n",
    "        desc = desc.lower()\n",
    "        return \"Y\" if any(keyword in desc for keyword in seasonal_keywords) else \"N\"\n",
    "\n",
    "    df[\"seasonal\"] = df[\"Description\"].apply(is_seasonal)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6755cd9-60db-4b27-9842-2bf5bd49fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def assign_product_category(df: pd.DataFrame, category_map: dict) -> pd.DataFrame:\n",
    "    category_keywords = {\n",
    "        cat: [kw.lower() for kw in keywords]\n",
    "        for cat, keywords in category_map.items()\n",
    "    }\n",
    "\n",
    "    def score_category(description):\n",
    "        if not isinstance(description, str):\n",
    "            return \"Unknown\"\n",
    "        description_lower = description.lower()\n",
    "\n",
    "        scores = {}\n",
    "        for category, keywords in category_keywords.items():\n",
    "            count = sum(1 for kw in keywords if re.search(rf\"\\b{re.escape(kw)}\\b\", description_lower))\n",
    "            if count > 0:\n",
    "                scores[category] = count\n",
    "\n",
    "        if scores:\n",
    "            return max(scores, key=scores.get)\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    df['product_category'] = df['Description'].apply(score_category)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7c819d-71e9-4e92-85fa-d51e59215d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_day(hour):\n",
    "        if 5 <= hour < 12:\n",
    "            return \"morning\"\n",
    "        elif 12 <= hour < 17:\n",
    "            return \"afternoon\"\n",
    "        elif 17 <= hour < 22:\n",
    "            return \"evening\"\n",
    "        else:\n",
    "            return \"night\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d40e-7a33-4196-9db2-36eefff5df04",
   "metadata": {},
   "source": [
    "Step 3: ETL before uploading the data into MSSQL server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84c6115-ce94-463f-9c9f-7de02df0bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def transform_fact_sales(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Mappings ===\n",
    "    continent_map = config[\"continent_map\"]\n",
    "    category_map = config[\"product_category\"]\n",
    "\n",
    "    country_to_continent = {\n",
    "        country: continent\n",
    "        for continent, countries in continent_map.items()\n",
    "        for country in countries\n",
    "    }\n",
    "\n",
    "    # === Drop records with null on Customer ID and Country ===#\n",
    "    df = df.dropna(subset=['Country', 'Customer_ID'])\n",
    "    \n",
    "    # === Fields for DimCustomer ===#\n",
    "    df['Customer_ID'] = df['Customer_ID'].fillna('Unknown')\n",
    "    df[\"customer_id\"] = pd.to_numeric(df[\"Customer_ID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df['country'] = df['Country'].fillna('Unknown')\n",
    "    df['continent'] = df['Country'].map(country_to_continent).fillna('Unknown')\n",
    "    # ==========================#\n",
    "\n",
    "    # === Fields for DimDate ===#\n",
    "    df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\") # -> This is useful to extract the targetted column values\n",
    "    df[\"date_id\"] = df[\"InvoiceDate\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    df[\"year\"] = df[\"InvoiceDate\"].dt.year\n",
    "    df[\"month\"] = df[\"InvoiceDate\"].dt.month\n",
    "    df[\"day\"] = df[\"InvoiceDate\"].dt.day\n",
    "    df[\"weekday\"] = df[\"InvoiceDate\"].dt.weekday + 1   # Monday = 1, Sunday = 7\n",
    "    df[\"quarter\"] = df[\"InvoiceDate\"].dt.quarter\n",
    "    # ==========================#\n",
    "\n",
    "    # === Fields for DimDate ===#\n",
    "    df[\"time_id\"] = df[\"InvoiceDate\"].dt.strftime(\"%H%M%S\").astype(int) \n",
    "    df[\"hours\"] = df[\"InvoiceDate\"].dt.hour\n",
    "    df[\"minutes\"] = df[\"InvoiceDate\"].dt.minute\n",
    "    df[\"seconds\"] = df[\"InvoiceDate\"].dt.second\n",
    "    df[\"time_of_day\"] = df[\"hours\"].apply(get_time_of_day)\n",
    "    # ==========================#\n",
    "\n",
    "    # === Fields for FactSales ===#\n",
    "    df[\"price\"] = df[\"Price\"].astype(float)\n",
    "    df[\"quantity\"] = df[\"Quantity\"].astype(int)\n",
    "    df[\"total_amount\"] = df[\"quantity\"] * df[\"price\"]\n",
    "\n",
    "    # === Fields for DimInvoice ===#\n",
    "    df[\"invoice_id\"] = df[\"Invoice\"].astype(str)    \n",
    "    df[\"returned\"] = df.apply(\n",
    "        lambda row: \"Y\" if row[\"Quantity\"] < 0 and str(row[\"invoice_id\"]).startswith(\"C\") else \"N\", axis=1)\n",
    "    df[\"discount\"] = df.apply(\n",
    "        lambda row: \"Y\" if row[\"Quantity\"] < 0 and row[\"StockCode\"] == \"D\" else \"N\", axis=1)\n",
    "    # =============================#\n",
    "    \n",
    "    df = df[df[\"Price\"] > 0]    \n",
    "\n",
    "    # === Fields for DimProduct ===#\n",
    "    df[\"product_id\"] = df[\"StockCode\"].astype(str)    \n",
    "    df[\"description\"] = df[\"Description\"].astype(str)\n",
    "    df = assign_product_category(df, category_map)\n",
    "    df = assign_seasonality_from_description(df)\n",
    "    # =============================#\n",
    "\n",
    "    df = df.drop(columns=[\"Invoice\", \"Customer_ID\", \"StockCode\", \"Quantity\", \"Country\", \"Price\", \"Description\"])#, \"InvoiceDate\"])\n",
    "\n",
    "    # Final filter\n",
    "    df_clean = df[\n",
    "        df[\"InvoiceDate\"].notnull() &\n",
    "        df[\"price\"].notnull() &\n",
    "        (df[\"quantity\"] != 0)\n",
    "    ].copy()\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ffa05-226e-4fc0-9183-f4f711a1a821",
   "metadata": {},
   "source": [
    "Step 4: Set Up SQL Server Connection & Load to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e32a41-6496-47f5-8731-5d6cb8d93d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_step\n",
    "def create_sql_engine_from_json(config: dict):\n",
    "\n",
    "    conn_params = config[\"conn_params\"]\n",
    "    username = conn_params[\"username\"]\n",
    "    password = conn_params[\"password\"]\n",
    "    server = conn_params[\"hostname\"]\n",
    "    database = conn_params[\"database\"]\n",
    "\n",
    "    # Create connection string\n",
    "    params = urllib.parse.quote_plus(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};\"\n",
    "        f\"DATABASE={database};\"\n",
    "        f\"UID={username};\"\n",
    "        f\"PWD={password}\"\n",
    "    )\n",
    "\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    return engine\n",
    "\n",
    "@log_step\n",
    "def load_table(df: pd.DataFrame, engine, table_name: str, columns: list[str] = None, if_exists: str = \"append\"):\n",
    "    upload_df = df.copy()\n",
    "\n",
    "    if columns:\n",
    "        upload_df = upload_df[columns]\n",
    "\n",
    "    upload_df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists=if_exists,\n",
    "        index=False,\n",
    "        chunksize=1000\n",
    "    )\n",
    "\n",
    "@log_step\n",
    "def truncate_tables(engine, tables: list[str], verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Truncate multiple tables in SQL Server.\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        for table in tables:\n",
    "            if verbose:\n",
    "                print(f\"Deleting: {table}\")\n",
    "            conn.execute(text(f\"DELETE FROM {table}\"))\n",
    "    print(\"All tables truncated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ba42f4-5145-448c-ae38-7d51ce62de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: load_json_config\n",
      "Completed: load_json_config in 0.00 seconds\n",
      "\n",
      "Starting: create_sql_engine_from_json\n",
      "Completed: create_sql_engine_from_json in 0.07 seconds\n",
      "\n",
      "Starting: extract_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_12000\\370155904.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: extract_data in 1.48 seconds\n",
      "\n",
      "Starting: transform_fact_sales\n",
      "Starting: assign_product_category\n",
      "Completed: assign_product_category in 150.70 seconds\n",
      "\n",
      "Starting: assign_seasonality_from_description\n",
      "Completed: assign_seasonality_from_description in 1.38 seconds\n",
      "\n",
      "Completed: transform_fact_sales in 169.91 seconds\n",
      "\n",
      "Starting: truncate_tables\n",
      "Deleting: FactSales_Staging\n",
      "Deleting: FactSales\n",
      "Deleting: DimCustomer\n",
      "Deleting: DimProduct\n",
      "Deleting: DimTime\n",
      "Deleting: DimDate\n",
      "Deleting: DimInvoice\n",
      "All tables truncated successfully.\n",
      "Completed: truncate_tables in 0.28 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 0.74 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 0.63 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 0.26 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 0.11 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 0.93 seconds\n",
      "\n",
      "Starting: load_table\n",
      "Completed: load_table in 231.13 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # LOAD CONFIG AND INITIALIZE ENGINE\n",
    "    # ---------------------------------------\n",
    "    config_file = load_json_config(\"./params.json\")\n",
    "    engine = create_sql_engine_from_json(config=config_file)\n",
    "    \n",
    "    # EXTRACT RAW DATA\n",
    "    # ---------------------------------------\n",
    "    df_raw = extract_data(\"./Invoices_Year_2009-2010.csv\")\n",
    "    \n",
    "    # TRANSFORM FACT SALES\n",
    "    # ---------------------------------------\n",
    "    df_fact_sales = transform_fact_sales(df=df_raw, config=config_file)\n",
    "\n",
    "    # TRUNCATE TABLES BEFORE LOAD PROCESS\n",
    "    # ---------------------------------------\n",
    "    tables_to_truncate = [\n",
    "        \"FactSales_Staging\",\n",
    "        \"FactSales\",\n",
    "        \"DimCustomer\",\n",
    "        \"DimProduct\",\n",
    "        \"DimTime\",\n",
    "        \"DimDate\",\n",
    "        \"DimInvoice\"\n",
    "    ]\n",
    "\n",
    "    truncate_tables(engine, tables_to_truncate)\n",
    "        \n",
    "    \n",
    "    # BUILD AND LOAD DIMENSION TABLES\n",
    "    # ---------------------------------------    \n",
    "    # Customer Dimension\n",
    "    df_dim_customer = df_fact_sales[['customer_id', 'country', 'continent']].drop_duplicates(subset=['customer_id'])\n",
    "    load_table(df_dim_customer, engine, \"DimCustomer\", columns=['customer_id', 'country', 'continent'])\n",
    "    \n",
    "    # Product Dimension\n",
    "    df_dim_product = df_fact_sales[['product_id', 'description', 'product_category', 'seasonal']].drop_duplicates(subset=['product_id'])\n",
    "    load_table(df_dim_product, engine, \"DimProduct\", columns=['product_id', 'description', 'product_category', 'seasonal'])\n",
    "    \n",
    "    # Time Dimension\n",
    "    df_dim_time = df_fact_sales[['time_id', 'hours', 'minutes', 'seconds', 'time_of_day']].drop_duplicates(subset=['time_id'])\n",
    "    load_table(df_dim_time, engine, \"DimTime\", columns=['time_id', 'hours', 'minutes', 'seconds', 'time_of_day'])\n",
    "    \n",
    "    # Date Dimension\n",
    "    df_dim_date = df_fact_sales[['date_id', 'year', 'month', 'day', 'weekday', 'quarter']].drop_duplicates(subset=['date_id'])\n",
    "    load_table(df_dim_date, engine, \"DimDate\", columns=['date_id', 'year', 'month', 'day', 'weekday', 'quarter'])\n",
    "    \n",
    "    # Invoice Dimension\n",
    "    df_dim_invoice = df_fact_sales[['invoice_id', 'returned', 'discount']].drop_duplicates(subset=['invoice_id'])\n",
    "    load_table(df_dim_invoice, engine, \"DimInvoice\", columns=['invoice_id', 'returned', 'discount'])\n",
    "    \n",
    "    # STAGING FACT TABLE (NATURAL KEYS)\n",
    "    # ---------------------------------------\n",
    "    df_staging = df_fact_sales[[\n",
    "        'invoice_id', 'product_id', 'customer_id',\n",
    "        'date_id', 'time_id', 'quantity', 'price', 'total_amount'\n",
    "    ]].drop_duplicates(subset=['invoice_id', 'product_id', 'date_id', 'time_id'])\n",
    "    \n",
    "    load_table(\n",
    "        df_staging,\n",
    "        engine,\n",
    "        table_name=\"FactSales_Staging\",\n",
    "        columns=[\n",
    "            'invoice_id', 'product_id', 'customer_id',\n",
    "            'date_id', 'time_id', 'quantity', 'price', 'total_amount'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # LOAD FACT TABLE WITH FOREIGN KEYS\n",
    "    # ---------------------------------------\n",
    "    insert_fact_sales_query = text(\"\"\"\n",
    "    INSERT INTO FactSales (\n",
    "        invoice_fk, \n",
    "        product_fk, \n",
    "        customer_fk, \n",
    "        date_fk, \n",
    "        time_fk, \n",
    "        quantity, \n",
    "        price, \n",
    "        total_amount\n",
    "    )\n",
    "    SELECT \n",
    "        i.invoice_pk, \n",
    "        p.product_pk, \n",
    "        c.customer_pk, \n",
    "        d.date_pk, \n",
    "        t.time_pk,\n",
    "        s.quantity, \n",
    "        s.price, \n",
    "        s.total_amount\n",
    "    FROM FactSales_Staging s\n",
    "    JOIN DimInvoice i  ON s.invoice_id = i.invoice_id\n",
    "    JOIN DimProduct p  ON s.product_id = p.product_id\n",
    "    JOIN DimCustomer c ON s.customer_id = c.customer_id\n",
    "    JOIN DimDate d     ON s.date_id = d.date_id\n",
    "    JOIN DimTime t     ON s.time_id = t.time_id;\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(insert_fact_sales_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36426b43-d926-4997-89b6-b46c7dac86a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
